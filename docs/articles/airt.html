<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Introduction to airt • airt</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Introduction to airt">
<meta property="og:description" content="airt">
<meta property="og:image" content="/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">airt</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.2.2</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../articles/airt.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Introduction to airt</h1>
            
      
      
      <div class="hidden name"><code>airt.Rmd</code></div>

    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://sevvandi.github.io/airt/" class="external-link">airt</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org" class="external-link">ggplot2</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyr.tidyverse.org" class="external-link">tidyr</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">gridExtra</span><span class="op">)</span></span></code></pre></div>
<p>The goal of <em>airt</em> is to evaluate the performance of a
portfolio of algorithms using Item Response Theory (IRT). The IRT models
are fitted using the R packages <strong>EstCRM</strong> and
<strong>mirt</strong>. The function in <strong>EstCRM</strong> is
slightly modified to account for a broader set of parameters.</p>
<div class="section level2">
<h2 id="classification-algorithms">Classification Algorithms<a class="anchor" aria-label="anchor" href="#classification-algorithms"></a>
</h2>
<p>This example is on classification algorithms. The data
<em>classification</em> has performance data from 10 classification
algorithms on 235 datasets. This data is discussed in <span class="citation">(Muñoz et al. 2018)</span> and can be found at the test
instance library MATILDA <span class="citation">(Smith-Miles
2019)</span>. Let’s have a look at this dataset.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"classification_cts"</span><span class="op">)</span></span>
<span><span class="va">df</span> <span class="op">&lt;-</span> <span class="va">classification_cts</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">df</span><span class="op">)</span></span>
<span><span class="co">#&gt;          NB       LDA       QDA      CART       J48       KNN     L_SVM</span></span>
<span><span class="co">#&gt; 1 0.7199042 0.7602850 0.7459878 0.7546605 0.7435086 0.7430308 0.7694158</span></span>
<span><span class="co">#&gt; 2 0.8358182 0.8404234 0.1045281 0.8270254 0.8347175 0.8328870 0.8284820</span></span>
<span><span class="co">#&gt; 3 0.8581818 0.8763636 0.8300000 0.8372727 0.8672727 0.8218182 0.8763636</span></span>
<span><span class="co">#&gt; 4 0.7467141 0.7356707 0.4451558 0.7356707 0.7356707 0.7530488 0.7356707</span></span>
<span><span class="co">#&gt; 5 0.9650329 0.1408706 0.1408706 0.9397915 0.9619915 0.9277021 0.9647557</span></span>
<span><span class="co">#&gt; 6 0.7739130 0.8536232 0.5060660 0.8536232 0.8507246 0.8391304 0.8521739</span></span>
<span><span class="co">#&gt;    poly_SVM   RBF_SVM     RandF</span></span>
<span><span class="co">#&gt; 1 0.7225890 0.7788021 0.7655677</span></span>
<span><span class="co">#&gt; 2 0.8212273 0.8300809 0.1045281</span></span>
<span><span class="co">#&gt; 3 0.8763636 0.8672727 0.8672727</span></span>
<span><span class="co">#&gt; 4 0.7704268 0.7685976 0.7557249</span></span>
<span><span class="co">#&gt; 5 0.7341198 0.8483237 0.1408706</span></span>
<span><span class="co">#&gt; 6 0.8579710 0.8521739 0.8637681</span></span></code></pre></div>
<p>In this dataset the columns represent algorithms and rows represent
datasets/instances. The values are performance values. That is, the
performance of dataset1 to algorithm Naive Bayes (NB) is 0.7199042. This
dataframe is the input to our AIRT model. We fit it by calling
<em>cirtmodel</em>.</p>
</div>
<div class="section level2">
<h2 id="fitting-a-continuous-irt-model-to-algorithm-performance-data">Fitting a Continuous IRT Model to Algorithm Performance Data<a class="anchor" aria-label="anchor" href="#fitting-a-continuous-irt-model-to-algorithm-performance-data"></a>
</h2>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">modout</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cirtmodel.html">cirtmodel</a></span><span class="op">(</span><span class="va">df</span><span class="op">)</span></span></code></pre></div>
<p>Now the model is fitted. Let’s have a look at traditional IRT
parameters.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">paras</span> <span class="op">&lt;-</span> <span class="va">modout</span><span class="op">$</span><span class="va">model</span><span class="op">$</span><span class="va">param</span></span>
<span><span class="va">paras</span></span>
<span><span class="co">#&gt;                    a           b      alpha</span></span>
<span><span class="co">#&gt; NB       1.027368059  -1.1185870 1.05927334</span></span>
<span><span class="co">#&gt; LDA      0.697568059  -1.9584262 0.94906831</span></span>
<span><span class="co">#&gt; QDA      0.008604556 -37.6665493 0.01731587</span></span>
<span><span class="co">#&gt; CART     1.598441089  -1.0209521 1.41547455</span></span>
<span><span class="co">#&gt; J48      1.558295477  -1.1640940 1.52036690</span></span>
<span><span class="co">#&gt; KNN      1.796892905  -0.8412235 1.64579669</span></span>
<span><span class="co">#&gt; L_SVM    2.846510834  -1.4371875 1.50572702</span></span>
<span><span class="co">#&gt; poly_SVM 1.743909296  -1.1499008 1.31318614</span></span>
<span><span class="co">#&gt; RBF_SVM  3.766472502  -1.4019959 1.53615811</span></span>
<span><span class="co">#&gt; RandF    0.999442464  -1.7509568 1.43550771</span></span></code></pre></div>
<p>The parameter <em>a</em> denotes discrimination, <em>b</em> denotes
difficulty and <em>alpha</em> is a scaling parameter. These are
traditional IRT parameters. Using these parameters we will find AIRT
algorithm attributes. These are <strong>algorithm anomalousness,
consistency and the difficulty limit</strong>.</p>
<p>If an algorithm is anomalous then the anomalous indicator is 1. In
this algorithm portfolio, none of the algorithms are anomalous, because
all anomalous indicators are 0. Anomalous algorithms give good
performances for difficult problems and poor performances for easy
problems.</p>
<p>The difficulty limit gives the highest difficulty level that
algorithms can handle. In this scenario, QDA has the highest difficulty
limit. So, QDA can handle the hardest problems. KNN has the lowest
difficulty limit. It can only handle very easy problems.</p>
<p>Algorithm consistency attribute gives how consistent an algorithm is.
An algorithm can be consistently good for most of the problems or it can
be consistently poor for many problems. And many algorithms can vary in
their performance depending on the problem/dataset. In this portfolio,
QDA is the most consistent algorithm.</p>
<p>Let’s look at these algorithms visually. The <em>heatmaps_crm</em>
function plots the heatmaps. The part <em>crm</em> stands for continuous
response model.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">obj</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/heatmaps_crm.html">heatmaps_crm</a></span><span class="op">(</span><span class="va">modout</span><span class="op">)</span> </span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/autoplot.html" class="external-link">autoplot</a></span><span class="op">(</span><span class="va">obj</span><span class="op">)</span></span></code></pre></div>
<p><img src="airt_files/figure-html/heatmaps-1.png" width="768">
Let’s discuss these heatmaps. Theta (x axis) represents the dataset
easiness and z (y axis) represents the normalized performance values.
The heatmaps show the probability density of the fitted IRT model over
Theta and z values for each algorithm.</p>
<p>Apart from QDA all heatmaps have a line (a bit like a lightsaber)
going through it. If the lightsaber has a positive slope, then the
algorithm is not anomalous. We see some lightsabers are sharper than
others. Algorithms with sharper lightsabers are more discriminating. The
algorithms with no lightsabers (QDA) or blurry lightsabers are more
consistent. In this portfolio, QDA is the most consistent as it doesn’t
have any lightsabers. LDA and NB are also somewhat consistent. RBF_SVM
is the least consistent (most discriminating) as it has a very sharp
line.</p>
<div class="section level3">
<h3 id="the-problem-difficulty-space-and-algorithm-performance">The Problem Difficulty Space and Algorithm Performance<a class="anchor" aria-label="anchor" href="#the-problem-difficulty-space-and-algorithm-performance"></a>
</h3>
<p>We can also look at the algorithm performance with respect to the
dataset difficulty. This is called the latent trait analysis. The
function <em>latent_trait_analysis</em> does this for you. We need to
pass the IRT parameters to do this analysis.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">obj</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/latent_trait_analysis.html">latent_trait_analysis</a></span><span class="op">(</span><span class="va">df</span>, <span class="va">modout</span><span class="op">$</span><span class="va">model</span><span class="op">$</span><span class="va">param</span>, epsilon <span class="op">=</span> <span class="fl">0</span> <span class="op">)</span></span>
<span><span class="co">#&gt; Joining with `by = join_by(group)`</span></span>
<span><span class="co">#&gt; Joining with `by = join_by(group)`</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/autoplot.html" class="external-link">autoplot</a></span><span class="op">(</span><span class="va">obj</span>, plottype <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<p><img src="airt_files/figure-html/latenttrait-1.png" width="768"></p>
<p>When you use <em>plottype = 1</em>, it plots all algorithms in a
single plot. To have a separate plot for each algorithm we use
<em>plottype = 2</em>.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/autoplot.html" class="external-link">autoplot</a></span><span class="op">(</span><span class="va">obj</span>, plottype <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<p><img src="airt_files/figure-html/latent2-1.png" width="768">
From these plots we see that certain algorithms give better performances
for different problem difficulty values. To get a better sense of which
algorithms are better for which difficulty values we fit smoothing
splines to the above data. By using <em>plottype = 3</em> in
<em>autoplot</em> we can see these smoothing splines.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/autoplot.html" class="external-link">autoplot</a></span><span class="op">(</span><span class="va">obj</span>, plottype <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<p><img src="airt_files/figure-html/latent3-1.png" width="768"></p>
</div>
<div class="section level3">
<h3 id="strengths-and-weaknesses-of-algorithms">Strengths and Weaknesses of Algorithms<a class="anchor" aria-label="anchor" href="#strengths-and-weaknesses-of-algorithms"></a>
</h3>
<p>From this plot, we can get the best algorithm for a given problem
difficulty. We can use these splines to compute the proportion of the
latent trait spectrum occupied by each algorithm. We call this the
latent trait occupancy (LTO). These are strengths of algorithms.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">obj</span><span class="op">$</span><span class="va">strengths</span><span class="op">$</span><span class="va">proportions</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 5 × 4</span></span></span>
<span><span class="co">#&gt;   group Proportion algorithm colour </span></span>
<span><span class="co">#&gt;   <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>      <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>     <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>  </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">1</span>     2    0.362   J48       #D89000</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">2</span>     4    0.294   L_SVM     #39B600</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">3</span>    10    0.221   RBF_SVM   #FF62BC</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">4</span>     3    0.115   KNN       #A3A500</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">5</span>     7    0.008<span style="text-decoration: underline;">51</span> poly_SVM  #00B0F6</span></span></code></pre></div>
<p>The column <em>Proportion</em> gives the latent trait occupancy of
the algorithm. In this scenario, J48 has the highest latent trait
occupancy.</p>
<p>Similar to strengths, we can say an algorithm is weak if it has the
lowest performance for a given difficulty.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">obj</span><span class="op">$</span><span class="va">weakness</span><span class="op">$</span><span class="va">proportions</span></span>
<span><span class="co">#&gt; <span style="color: #949494;"># A tibble: 2 × 4</span></span></span>
<span><span class="co">#&gt;   group Proportion algorithm colour </span></span>
<span><span class="co">#&gt;   <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>      <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span> <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>     <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>  </span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">1</span>     8    0.996   QDA       #9590FF</span></span>
<span><span class="co">#&gt; <span style="color: #BCBCBC;">2</span>    10    0.004<span style="text-decoration: underline;">26</span> RBF_SVM   #FF62BC</span></span></code></pre></div>
<p>In this example QDA is the weakness algorithm. QDA is weak for 0.99
of the latent trait. But now there is a big question. If QDA is the
weakest algorithm, why did it have such a high difficulty limit? It had
the highest difficulty limit of all the algorithms. What happened
here?</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/autoplot.html" class="external-link">autoplot</a></span><span class="op">(</span><span class="va">obj</span>, plottype <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span></code></pre></div>
<p><img src="airt_files/figure-html/latent4-1.png" width="768"></p>
<p>We see latent trait occupancy in the graph above. The 5 algorithms
J48, KNN L_SVM, poly_SVM and RBF_SVM occupy parts of the latent trait
spectrum. That is, for some dataset easiness values, these algorithms
display superiority.</p>
<p>In this example we have used epsilon = 0. That would give a unique
strength/weakness to each point in the problem space. If we make epsilon
&gt; 0, then we can get overlapping strengths and weaknesses. That is,
we will get algorithms that are epsilon-away from the best algorithm in
our strengths/weakness diagram. Let’s do that.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">obj2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/latent_trait_analysis.html">latent_trait_analysis</a></span><span class="op">(</span><span class="va">df</span>, <span class="va">modout</span><span class="op">$</span><span class="va">model</span><span class="op">$</span><span class="va">param</span>, epsilon <span class="op">=</span> <span class="fl">0.02</span> <span class="op">)</span></span>
<span><span class="co">#&gt; Joining with `by = join_by(group)`</span></span>
<span><span class="co">#&gt; Joining with `by = join_by(group)`</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/autoplot.html" class="external-link">autoplot</a></span><span class="op">(</span><span class="va">obj2</span>, plottype <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span></code></pre></div>
<p><img src="airt_files/figure-html/latent5-1.png" width="768">
Now we see some overlapping strengths and weaknesses. For very easy
problems, many algorithms have strengths, and for more difficult
problems, we see that KNN and J48 are strong. QDA is weak for most part
of the problem space.</p>
</div>
<div class="section level3">
<h3 id="is-this-a-good-model-model-goodness-metrics">Is this a good model? Model Goodness Metrics<a class="anchor" aria-label="anchor" href="#is-this-a-good-model-model-goodness-metrics"></a>
</h3>
<p>All this is good, but is the fitted IRT model good? To check this, we
have a couple of measures. One is the Model Goodness Curve. We first
call the model_goodness_crm function to compute the model goodness
metrics. Then by calling autoplot we can plot the curves. The letters
<em>crm</em> stands for continuous response model.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="va">modelgood</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/model_goodness_crm.html">model_goodness_crm</a></span><span class="op">(</span><span class="va">modout</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/autoplot.html" class="external-link">autoplot</a></span><span class="op">(</span><span class="va">modelgood</span><span class="op">)</span></span></code></pre></div>
<p><img src="airt_files/figure-html/modelgoodness-1.png" width="768"></p>
<p>In the above graph, we’re looking at the distribution of errors –
that is, the difference between the predicted and the actual values for
different algorithms. The x-axis has the absolute error scaled to [0,1]
and the y-axis shows the empirical cumulative distribution of errors for
each algorithm. For a given algorithm a model is well fitted if the
curve goes up to 1 on the y-axis quickly. That is, if the Area Under the
Curve (AUC) is closer to 1. We can check the AUC and the Mean Square
Error (MSE) for these algorithms.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">cbind.data.frame</a></span><span class="op">(</span>AUC <span class="op">=</span> <span class="va">modelgood</span><span class="op">$</span><span class="va">goodnessAUC</span>, MSE <span class="op">=</span> <span class="va">modelgood</span><span class="op">$</span><span class="va">mse</span><span class="op">)</span></span>
<span><span class="co">#&gt;                AUC         MSE</span></span>
<span><span class="co">#&gt; NB       0.9398882 0.006986392</span></span>
<span><span class="co">#&gt; LDA      0.9153020 0.036052464</span></span>
<span><span class="co">#&gt; QDA      0.6251236 0.215338281</span></span>
<span><span class="co">#&gt; CART     0.9362777 0.005440992</span></span>
<span><span class="co">#&gt; J48      0.9260477 0.007232708</span></span>
<span><span class="co">#&gt; KNN      0.9226521 0.007537148</span></span>
<span><span class="co">#&gt; L_SVM    0.8984956 0.011709917</span></span>
<span><span class="co">#&gt; poly_SVM 0.9184827 0.008688169</span></span>
<span><span class="co">#&gt; RBF_SVM  0.8928648 0.012677875</span></span>
<span><span class="co">#&gt; RandF    0.8535783 0.033002535</span></span></code></pre></div>
<p>From the graph and the table we see that the IRT model fits all
algorithms well apart from QDA. We have another goodness metric called
effectiveness. Effectiveness generally tells us how good the algorithms
are.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">modeleff</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/effectiveness_crm.html">effectiveness_crm</a></span><span class="op">(</span><span class="va">modout</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/autoplot.html" class="external-link">autoplot</a></span><span class="op">(</span><span class="va">modeleff</span>, plottype <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<p><img src="airt_files/figure-html/effectiveness1-1.png" width="768">
This first plot tells us how good the algorithms actually perform,
without fitting an IRT model.</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/autoplot.html" class="external-link">autoplot</a></span><span class="op">(</span><span class="va">modeleff</span>, plottype <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<p><img src="airt_files/figure-html/effectiveness2-1.png" width="768"></p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/autoplot.html" class="external-link">autoplot</a></span><span class="op">(</span><span class="va">modeleff</span>, plottype <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<p><img src="airt_files/figure-html/effectiveness2-2.png" width="768">
By using plottype = 2, we can get the predicted effectiveness and by
using plottype = 3 we see how the actual and the predicted sit together.
Here AUPEC means Area Under Predicted Effectiveness Curve and AUAEC
means Area Under Actual Effectiveness Curve.</p>
<p>The IRT model has fitted the algorithms well if the points are close
to the y = x line, shown as a dotted line. Again we see that apart from
QDA, all algorithms are fitted well.</p>
</div>
</div>
<div class="section level2">
<h2 id="a-polytomous-irt-model">A Polytomous IRT model<a class="anchor" aria-label="anchor" href="#a-polytomous-irt-model"></a>
</h2>
<p>Polytomous is a fancy word. It means ordered data like <em>Very Good
&gt; Good &gt; Neutral &gt; Bad &gt; Very Bad</em>. We can think of it
as discrete data. We have binned the previous classification data to 5
binds so that we can fit a polytomous IRT model to it. The lowest
performance measurement is called P1 and the highest is called P5, with
the others in between. Let’s fit an IRT model to this data and look at
the algorithm trace lines.</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"classification_poly"</span><span class="op">)</span></span>
<span><span class="va">modout</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/pirtmodel.html">pirtmodel</a></span><span class="op">(</span><span class="va">classification_poly</span>, vpara<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="va">obj</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/tracelines_poly.html">tracelines_poly</a></span><span class="op">(</span><span class="va">modout</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/autoplot.html" class="external-link">autoplot</a></span><span class="op">(</span><span class="va">obj</span><span class="op">)</span></span></code></pre></div>
<p><img src="airt_files/figure-html/example-1.png" width="768"></p>
<p>The trace lines give the probability of getting performance levels
from P1 to P5, for different values of dataset easiness denoted by
Theta. Theta is dataset easiness. The probability of getting P5 is
higher for an easy dataset, while it is lower for a difficult dataset.
We see that some algorithms have only levels P3 to P5, while some have
all performance levels. Also, some algorithms like QDA have gentler
transitions between the most likely performance levels, and some like
RBF_SVM have very sharp transitions.</p>
<p>We can look at some of the AIRT attributes.</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">cbind.data.frame</a></span><span class="op">(</span>consistency <span class="op">=</span> <span class="va">modout</span><span class="op">$</span><span class="va">consistency</span>, anomalousness <span class="op">=</span> <span class="va">modout</span><span class="op">$</span><span class="va">anomalous</span>, difficulty_level <span class="op">=</span> <span class="va">modout</span><span class="op">$</span><span class="va">difficulty_limit</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="co">#&gt;          consistency anomalousness difficulty_level</span></span>
<span><span class="co">#&gt; NB        0.52836203             0        1.8606375</span></span>
<span><span class="co">#&gt; LDA       0.92172649             0        1.0267424</span></span>
<span><span class="co">#&gt; QDA       2.85474958             0       -0.3402693</span></span>
<span><span class="co">#&gt; CART      0.15935141             0        1.6779239</span></span>
<span><span class="co">#&gt; J48       0.15903700             0        1.6764064</span></span>
<span><span class="co">#&gt; KNN       0.14894278             0        1.6009962</span></span>
<span><span class="co">#&gt; L_SVM     0.16090048             0        2.0630631</span></span>
<span><span class="co">#&gt; poly_SVM  0.28803993             0        2.1199698</span></span>
<span><span class="co">#&gt; RBF_SVM   0.07016627             0        1.9192434</span></span>
<span><span class="co">#&gt; RandF     0.84845381             0        1.2070668</span></span></code></pre></div>
<div class="section level3">
<h3 id="model-goodness">Model Goodness<a class="anchor" aria-label="anchor" href="#model-goodness"></a>
</h3>
<p>But how good is our IRT model? Can we trust the algorithm trace
lines? To check how good the IRT model is we compute the goodness of
model in this way.</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="va">modelgoodness</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/model_goodness_poly.html">model_goodness_poly</a></span><span class="op">(</span><span class="va">modout</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/autoplot.html" class="external-link">autoplot</a></span><span class="op">(</span><span class="va">modelgoodness</span><span class="op">)</span></span></code></pre></div>
<p><img src="airt_files/figure-html/goodnesspoly-1.png" width="768">
We can see the predicted and actual effectivess as well.</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">effpoly</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/effectiveness_poly.html">effectiveness_poly</a></span><span class="op">(</span><span class="va">modout</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/autoplot.html" class="external-link">autoplot</a></span><span class="op">(</span><span class="va">effpoly</span>, plottype <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<p><img src="airt_files/figure-html/effectivenesspoly-1.png" width="768">
From this plot we see that again QDA is the outlier. Generally the AIRT
model predicts higher effectiveness (AUPEC) for almost all
algorithms.</p>
<p>More examples of algorithm evaluation using airt are discussed in our
paper <span class="citation">(Kandanaarachchi and Smith-Miles
2023)</span>.</p>
</div>
</div>
<div class="section level2 unnumbered">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-airtPaper" class="csl-entry">
Kandanaarachchi, Sevvandi, and Kate Smith-Miles. 2023.
<span>“Comprehensive Algorithm Portfolio Evaluation Using Item Response
Theory.”</span> <em>Journal of Machine Learning Research, Accepted</em>.
</div>
<div id="ref-munoz2018instance" class="csl-entry">
Muñoz, Mario A, Laura Villanova, Davaatseren Baatar, and Kate
Smith-Miles. 2018. <span>“Instance Spaces for Machine Learning
Classification.”</span> <em>Machine Learning</em> 107 (1): 109–47.
</div>
<div id="ref-matilda" class="csl-entry">
Smith-Miles, Kate. 2019. <em>MATILDA: Melbourne Algorithm Test Instance
Library with Data Analytics</em>. <a href="https://matilda.unimelb.edu.au/matilda/" class="external-link">https://matilda.unimelb.edu.au/matilda/</a>.
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Sevvandi Kandanaarachchi.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.6.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
